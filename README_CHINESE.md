# ç¹é«”ä¸­æ–‡æ”¯æ´ / Traditional Chinese Support

[English](#english) | [ç¹é«”ä¸­æ–‡](#ç¹é«”ä¸­æ–‡)

---

## ç¹é«”ä¸­æ–‡

### æ¦‚è¿°
TextReadingRAG ç³»çµ±ç¾å·²å…¨é¢æ”¯æ´ç¹é«”ä¸­æ–‡å’Œç°¡é«”ä¸­æ–‡æ–‡æª”æª¢ç´¢ã€‚ç³»çµ±æœƒè‡ªå‹•æª¢æ¸¬æ–‡æª”èªè¨€ï¼Œä¸¦æ‡‰ç”¨é‡å°æ€§çš„è™•ç†ç­–ç•¥ä»¥ç²å¾—æœ€ä½³æ•ˆæœã€‚

### ä¸»è¦åŠŸèƒ½

âœ… **è‡ªå‹•èªè¨€æª¢æ¸¬** - è‡ªå‹•è­˜åˆ¥è‹±æ–‡å’Œä¸­æ–‡æ–‡æª”
âœ… **ä¸­æ–‡åˆ†è©** - ä½¿ç”¨ jieba é€²è¡Œæº–ç¢ºçš„ä¸­æ–‡åˆ†è©
âœ… **èªç¾©åˆ†å¡Š** - å°Šé‡ä¸­æ–‡å¥å­é‚Šç•Œï¼ˆã€‚ï¼ï¼Ÿï¼‰
âœ… **å„ªåŒ–çš„åˆ†å¡Šå¤§å°** - é‡å°ä¸­æ–‡ä¿¡æ¯å¯†åº¦å„ªåŒ–ï¼ˆ256å­—ç¬¦ vs è‹±æ–‡512å­—ç¬¦ï¼‰
âœ… **æ··åˆæª¢ç´¢** - çµåˆå‘é‡æª¢ç´¢å’Œ BM25 ç¨€ç–æª¢ç´¢
âœ… **å¤šèªè¨€æŸ¥è©¢** - æ”¯æŒä¸­è‹±æ–‡æŸ¥è©¢

### å¿«é€Ÿé–‹å§‹

#### 1. å®‰è£ä¾è³´
```bash
pip install -r requirements.txt
```

æ–°å¢çš„ä¸­æ–‡æ”¯æ´ä¾è³´ï¼š
- `jieba>=0.42.1` - ä¸­æ–‡åˆ†è©
- `opencc-python-reimplemented>=0.1.7` - ç¹ç°¡è½‰æ›æ”¯æ´

#### 2. é…ç½®

åœ¨ `.env` æ–‡ä»¶ä¸­æ·»åŠ ï¼š
```bash
# å•Ÿç”¨èªè¨€æª¢æ¸¬
ENABLE_LANGUAGE_DETECTION=True

# ä¸­æ–‡åˆ†å¡Šåƒæ•¸
CHINESE_CHUNK_SIZE=256
CHINESE_CHUNK_OVERLAP=64

# æŸ¥è©¢æ“´å±•ï¼ˆä½¿ç”¨ LLM ä»¥æ”¯æŒä¸­æ–‡ï¼‰
QUERY_EXPANSION_METHODS="llm"
```

#### 3. ä½¿ç”¨ç¤ºä¾‹

**ä¸Šå‚³ä¸­æ–‡æ–‡æª”ï¼š**
```python
POST /api/v1/documents/upload
Content-Type: multipart/form-data

file: "ç¹é«”ä¸­æ–‡æ–‡æª”.pdf"
```

**ä¸­æ–‡æŸ¥è©¢ï¼š**
```python
POST /api/v1/query
{
  "query": "ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’ï¼Ÿ",
  "top_k": 5
}
```

### æŠ€è¡“ç‰¹é»

#### è‹±æ–‡è™•ç†
- åˆ†è©ï¼šç©ºæ ¼åˆ†å‰²
- åˆ†å¡Šå¤§å°ï¼š512å­—ç¬¦
- å¥å­é‚Šç•Œï¼š. ! ?
- æŸ¥è©¢æ“´å±•ï¼šLLM + åŒç¾©è©

#### ä¸­æ–‡è™•ç†
- åˆ†è©ï¼šjieba åˆ†è©
- åˆ†å¡Šå¤§å°ï¼š256å­—ç¬¦
- å¥å­é‚Šç•Œï¼šã€‚ï¼ï¼Ÿ
- æŸ¥è©¢æ“´å±•ï¼šåƒ… LLM

### ç¤ºä¾‹ä»£ç¢¼

```python
from src.rag.language_utils import detect_language, split_chinese_text

# æª¢æ¸¬èªè¨€
text = "é€™æ˜¯ç¹é«”ä¸­æ–‡æ¸¬è©¦æ–‡æœ¬ã€‚"
language = detect_language(text)  # è¿”å›: 'zh'

# åˆ†å‰²ä¸­æ–‡æ–‡æœ¬
chunks = split_chinese_text(
    text,
    chunk_size=256,
    chunk_overlap=64
)
```

### é‹è¡Œç¤ºä¾‹

```bash
# é‹è¡Œä¸­æ–‡æ”¯æ´ç¤ºä¾‹
python examples/chinese_example.py

# é‹è¡Œæ¸¬è©¦
pytest tests/test_rag/test_chinese_support.py -v
```

### æ–‡æª”

- **ğŸ“– æ–‡æª”ä¸­å¿ƒ**: [docs/README.md](docs/README.md) - å®Œæ•´æ–‡æª”å°è¦½
- **å®Œæ•´æŒ‡å—**: [docs/guides/CHINESE_SUPPORT.md](docs/guides/CHINESE_SUPPORT.md) - åŒ…å«å¿«é€Ÿé–‹å§‹èˆ‡è©³ç´°èªªæ˜
- **ç¤ºä¾‹ä»£ç¢¼**: [examples/chinese_example.py](examples/chinese_example.py)

### æ€§èƒ½

- èªè¨€æª¢æ¸¬ï¼š~1-5æ¯«ç§’/æ–‡æª”
- jieba åˆ†è©ï¼š~10-50æ¯«ç§’/æ–‡æª”
- å°è‹±æ–‡æ–‡æª”ç„¡å½±éŸ¿
- å…§å­˜é–‹éŠ·ï¼š~30MBï¼ˆjieba è©å…¸ï¼Œå…±äº«ï¼‰

### æ”¯æ´çš„èªè¨€

| èªè¨€ | ä»£ç¢¼ | åˆ†è©æ–¹å¼ | åˆ†å¡Šå¤§å° |
|------|------|----------|----------|
| è‹±æ–‡ | en | ç©ºæ ¼åˆ†å‰² | 512å­—ç¬¦ |
| ç¹é«”ä¸­æ–‡ | zh | jieba | 256å­—ç¬¦ |
| ç°¡é«”ä¸­æ–‡ | zh | jieba | 256å­—ç¬¦ |

---

## English

### Overview
TextReadingRAG now fully supports Traditional Chinese (ç¹é«”ä¸­æ–‡) and Simplified Chinese (ç®€ä½“ä¸­æ–‡) document retrieval. The system automatically detects document language and applies language-specific processing for optimal results.

### Key Features

âœ… **Automatic Language Detection** - Auto-identifies English and Chinese documents
âœ… **Chinese Tokenization** - Uses jieba for accurate Chinese word segmentation
âœ… **Semantic Chunking** - Respects Chinese sentence boundaries (ã€‚ï¼ï¼Ÿ)
âœ… **Optimized Chunk Sizes** - Optimized for Chinese information density (256 vs 512 chars)
âœ… **Hybrid Retrieval** - Combines vector search and BM25 sparse retrieval
âœ… **Multilingual Queries** - Supports both English and Chinese queries

### Quick Start

#### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

New Chinese support dependencies:
- `jieba>=0.42.1` - Chinese word segmentation
- `opencc-python-reimplemented>=0.1.7` - Traditional/Simplified conversion support

#### 2. Configuration

Add to your `.env` file:
```bash
# Enable language detection
ENABLE_LANGUAGE_DETECTION=True

# Chinese chunking parameters
CHINESE_CHUNK_SIZE=256
CHINESE_CHUNK_OVERLAP=64

# Query expansion (use LLM for multilingual support)
QUERY_EXPANSION_METHODS="llm"
```

#### 3. Usage Examples

**Upload Chinese Document:**
```python
POST /api/v1/documents/upload
Content-Type: multipart/form-data

file: "traditional_chinese_doc.pdf"
```

**Chinese Query:**
```python
POST /api/v1/query
{
  "query": "What is machine learning?",
  "top_k": 5
}
```

### Technical Details

#### English Processing
- Tokenization: Whitespace splitting
- Chunk size: 512 characters
- Sentence boundaries: . ! ?
- Query expansion: LLM + Synonym

#### Chinese Processing
- Tokenization: jieba word segmentation
- Chunk size: 256 characters
- Sentence boundaries: ã€‚ï¼ï¼Ÿ
- Query expansion: LLM only

### Example Code

```python
from src.rag.language_utils import detect_language, split_chinese_text

# Detect language
text = "é€™æ˜¯ç¹é«”ä¸­æ–‡æ¸¬è©¦æ–‡æœ¬ã€‚"
language = detect_language(text)  # Returns: 'zh'

# Split Chinese text
chunks = split_chinese_text(
    text,
    chunk_size=256,
    chunk_overlap=64
)
```

### Run Examples

```bash
# Run Chinese support examples
python examples/chinese_example.py

# Run tests
pytest tests/test_rag/test_chinese_support.py -v
```

### Documentation

- **ğŸ“– Documentation Center**: [docs/README.md](docs/README.md) - Complete documentation index
- **Complete Guide**: [docs/guides/CHINESE_SUPPORT.md](docs/guides/CHINESE_SUPPORT.md) - Quick start + full documentation
- **Example Code**: [examples/chinese_example.py](examples/chinese_example.py)

### Performance

- Language detection: ~1-5ms per document
- jieba tokenization: ~10-50ms per document
- No impact on English documents
- Memory overhead: ~30MB (jieba dictionary, shared)

### Supported Languages

| Language | Code | Tokenization | Chunk Size |
|----------|------|--------------|------------|
| English | en | Whitespace | 512 chars |
| Traditional Chinese | zh | jieba | 256 chars |
| Simplified Chinese | zh | jieba | 256 chars |

### Architecture

```
Document Upload â†’ Language Detection â†’ Language-Specific Processing
                                               â†“
                     English â†â†’ [Router] â†â†’ Chinese
                        â†“                      â†“
                  SentenceSplitter      jieba + Boundaries
                        â†“                      â†“
                        â””â”€â”€â†’ Embeddings â†â”€â”€â”€â”€â”€â”€â”˜
                                â†“
                           ChromaDB Storage

Query â†’ Detection â†’ Multilingual BM25 + Dense Retrieval â†’ Results
```

### Testing

```bash
# All tests
pytest tests/test_rag/test_chinese_support.py -v

# Specific test class
pytest tests/test_rag/test_chinese_support.py::TestLanguageDetection -v

# Integration tests
pytest tests/test_rag/test_chinese_support.py::TestIntegration -v
```

### Resources

- **jieba**: https://github.com/fxsjy/jieba
- **OpenAI Embeddings**: Supports multilingual (including Chinese)
- **LangDetect**: Language detection library

### Support

For issues or questions:
1. Check documentation: [docs/guides/CHINESE_SUPPORT.md](docs/guides/CHINESE_SUPPORT.md)
2. Review test cases: [tests/test_rag/test_chinese_support.py](tests/test_rag/test_chinese_support.py)
3. Run examples: `python examples/chinese_example.py`
4. ChromaDB troubleshooting: [docs/troubleshooting/TROUBLESHOOTING_CHROMADB.md](docs/troubleshooting/TROUBLESHOOTING_CHROMADB.md)

---

## æŠ€è¡“æ”¯æŒ / Technical Support

- ğŸ“– æ–‡æª”ä¸­å¿ƒ / Documentation Center: [docs/README.md](docs/README.md)
- ğŸ“– å®Œæ•´æŒ‡å— / Complete Guide: [docs/guides/CHINESE_SUPPORT.md](docs/guides/CHINESE_SUPPORT.md)
- ğŸ’» ç¤ºä¾‹ä»£ç¢¼ / Examples: [examples/chinese_example.py](examples/chinese_example.py)
- ğŸ§ª æ¸¬è©¦ / Tests: [tests/test_rag/test_chinese_support.py](tests/test_rag/test_chinese_support.py)
